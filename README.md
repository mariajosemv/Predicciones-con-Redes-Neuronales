# üß†üîÄ Predicciones a trav√©s de redes neuronales artificiales

Inspirada en la naturaleza de las redes neuronales de nuestro cerebro, en los siguientes proyectos se realizan predicciones tomando como entrada los datos de inter√©s. El algoritmo implementado en cada proyecto es organizado en diferentes n√∫meros de capas que representan ciertas funciones matem√°ticas. En cada capa se reconocen patrones y caracter√≠sticas que permiten, en √∫ltima instancia, entregar una predicci√≥n con un cierto nivel de confianza. 

Los proyectos son presentados de acuerdo al orden de complejidad de la red neuronal y se ordenan de la siguiente manera:

- 1Ô∏è‚É£ Implementaci√≥n de un perceptr√≥n (red neuronal b√°sica) para un problema de clasificaci√≥n binaria: discernir entre especies de flores.

- 2Ô∏è‚É£ Dise√±o, entrenamiento y evaluaci√≥n de una red neuronal multicapa para un problema de regresi√≥n: predecir precios de viviendas en Boston.

- 3Ô∏è‚É£ Dise√±o, entrenamiento y evaluaci√≥n de una red neuronal multicapa para un problema de clasificaci√≥n: identificar clientes potenciales. 

- 4Ô∏è‚É£ Introducci√≥n al Deep Learning: dise√±o, entrenamiento y evaluaci√≥n de una red neuronal para determinar el precio justo para venta de veh√≠culos usados. 


## üåü Funcionamiento

El algoritmo de una red neuronal funciona de acuerdo los siguientes pasos:

- 1Ô∏è‚É£ Inicializaci√≥n de los pesos sin√°pticos.
- 2Ô∏è‚É£ Definici√≥n de la funci√≥n de activaci√≥n para abstraer relaciones no lineales. 
- 3Ô∏è‚É£ Aplicar funciones de costo para medir la diferencia entre el valor predicho y el valor real.
- 4Ô∏è‚É£ Aplicar el algoritmo de optimizaci√≥n para reducir el error o funci√≥n de costo.
- 5Ô∏è‚É£ Ajustar los pesos sin√°pticos de acuerdo al paso 4.

### üîª Inicializaci√≥n de los pesos

Los rangos de los pesos de inicializaci√≥n que han demostrado ser m√°s efectivos y que son utilizados en este proyecto, est√°n definidos de acuerdo a las variaciones de Glorot/Xavier: 

- Intervalo Uniforme [-x,x]:

    ![equation](https://latex.codecogs.com/gif.latex?x%20%3D%20%5Csqrt%7B%20%5Cfrac%7B6%7D%7BE%20&plus;%20S%7D%20%7D)

- Intervalo Normal: con Media 0 y œÉ:

    ![equation](https://latex.codecogs.com/gif.latex?%5Csigma%20%3D%20%5Csqrt%7B%20%5Cfrac%7B2%7D%7BE%20&plus;%20S%7D%20%7D)

donde E y S son la cantidad de entradas y salidas. 

### üîª Funciones de activaci√≥n

Algunas de las funciones m√°s utilizadas son: 

- Funci√≥n Sigmoid o Log√≠stica: Es utilizada especialmente para los modelos en los que tenemos que predecir la probabilidad como un resultado ya que tiene una salida multivalor acotada de (0,1). 

    ![equation](https://latex.codecogs.com/gif.latex?f%28x%29%20%3D%20%5Cfrac%7B1%7D%7B1&plus;e%5E%7B-x%7D%7D)

- Funci√≥n Tangente Hiperb√≥lica: es usualmente usada en problemas de clasificaci√≥n binaria, con una salida acotada entre (-1,1).

    ![equation](https://latex.codecogs.com/gif.latex?f%28x%29%20%3D%20%5Cfrac%7Be%5Ex%20-%20e%5E%7B-x%7D%7D%7Be%5Ex%20&plus;%20e%5E%7B-x%7D%7D)


- ReLU (Rectified Linear Unit): tiene salida no acotada (0,‚àû) y derivadas positivas, por lo que es importante considerar las variables de entrada normalizadas (de 0 a 1). Es usada en problemas de regresi√≥n en los que se entrega una n√∫mero final. 
    
    ![equation](https://latex.codecogs.com/gif.latex?f%28x%29%20%3D%20%5Ctext%7Bmax%7D%20%5C%7B%200%2Cx%20%5C%7D)


### üîª Funciones de costo

- Para variables num√©ricas: 

    - Mean Absolute Error: es de f√°cil interpretaci√≥n y robusta a outliers. 
    
        ![equation](https://latex.codecogs.com/gif.latex?MAE%20%3D%20%5Cfrac%7B1%7D%7Bk%7D%20%5Csum_i%5Ek%20%7CReal%20-%20Predicho%7C)

    - Mean Squared Error: penaliza el modelo cuando existen grandes errores y es sensible a outliers.
    
        ![equation](https://latex.codecogs.com/gif.latex?MSE%20%3D%20%5Cfrac%7B1%7D%7Bk%7D%20%5Csum_i%5Ek%20%28Real%20-%20Predicho%29%5E2)

    - Mean Absolute Percentage Error: penaliza el modelo cuando existen grandes errores y es robusta a outliers. 
        
        ![equation](https://latex.codecogs.com/gif.latex?MAPE%20%3D%20%5Cfrac%7B1%7D%7Bk%7D%20%5Csum_i%5Ek%20%7C%20%5Cfrac%7BReal%20-%20Predicho%7D%7BReal%7D%7C)

- Para variables categ√≥ricas:

  - Binary Cross-Entropy: Penaliza el modelo cuando existen grandes errores. En la siguiente ecuaci√≥n, ![equation](https://latex.codecogs.com/gif.latex?y%27_%7Bi%7D) es el valor predicho y ![equation](https://latex.codecogs.com/gif.latex?y_%7Bi%7D) es el valor real.
  
    ![equation](https://latex.codecogs.com/gif.latex?H_%7By%27%7D%28y%29%20%3A%3D%20-%20%5Csum_%7Bi%7D%20%28%7By_i%27%20%5Clog%28y_i%29%20&plus;%20%281-y_i%27%29%20%5Clog%20%281-y_i%29%7D%29)
  
  - Categorical Cross-Entropy: Usada en problemas multiclase, de igual manera penaliza el modelo cuando presenta grandes errores. En la siguiente ecuaci√≥n p(x) es el valor real, y q(x) el valor predicho. 
  
    ![equation](https://latex.codecogs.com/gif.latex?H%28p%2Cq%29%20%3D%20-%5Csum_%7B%5Cforall%20x%7D%20p%28x%29%20%5Clog%28q%28x%29%29)

### üîª Algoritmos de optimizaci√≥n

- Gradiente descendiente: Calcula c√≥mo deben ser alterados los pesos para que la funci√≥n de coste pueda alcanzar un m√≠nimo, la desventaja es que este c√°lculo se aplica sobre todo el dataset, por lo que toma mucho coste computacional y es posible que el gradiente se ubique solo un m√≠nimo local.

- Gradiente descendiente estoc√°stico (SGD): es una variaci√≥n del gradiente descendiente en donde los pesos son modificados en cada lote (`batch`) de informaci√≥n. Presenta la ventaja de tener un coste computacional menor pero a cambio, los par√°metros del modelo pueden tener gran varianza debido a la frecuencia de la actualizaci√≥n de los pesos.  

- Momentum: fue dise√±ado para reducir la varianza en del SGD, al acelerar la convergencia hacia la direcci√≥n relevante y reducir la fluctuaci√≥n en la direcci√≥n irrelevante. 

- Gradiente adaptativo (AdaGrad): sigue el mismo principio que el algoritmo SGD, solo que las actualizaciones de los pesos es independiente uno del otro. Esto implica que cada peso empezar√° a obtener su valor y sucesivamente en alg√∫n momento en el tiempo se encuentra la soluci√≥n global.

- ADAM (AdaGrad+Momentum): incorpora los principios anteriores para acelerar el descenso del gradiente al considerar pasos peque√±os pero directos hacia la direcci√≥n correspondiente al m√≠nimo.

## ‚úÖ Evaluaci√≥n

El algorimo aplicado en cada red neuronal representa un modelo de los datos, para determinar si el modelo logra generalizar su comportamiento se deben considerar:

- 1Ô∏è‚É£ M√©tricas de desempe√±o.
- 2Ô∏è‚É£ Curvas de aprendizaje.


### üîª M√©tricas de desempe√±o

- Para problemas de regresi√≥n: como en estos problemas la variable de salida es de tipo num√©rico, se pueden aplicar las mismas funciones de costo que se han definido anteriormente, es decir, `MAE`, `MSE` o `MAPE`, por mencionar algunos ejemplos. 

- Para problemas de clasifiaci√≥n: en estos casos el modelo arroja un resultado Positivo o Negativo, que luego ser√° evaluado como Verdadero o Falso dependiendo de si la predicci√≥n sea correcta o no. Los posibles resultados de un problema de clasificaci√≥n se etiquetan como: 

  - *VP*: verdaderos positivos, 
  - *VN*: verdaderos negativos, 
  - *FP*: falsos positivos,
  - *FN*: falsos negativos,

  A partir de este conteo se definen las siguientes m√©tricas: 

  - Accuracy: 
  
    ![equation](https://latex.codecogs.com/gif.latex?%5Cfrac%7BVP&plus;VN%7D%7BVP&plus;VN&plus;FP&plus;FN%7D)
    
  - Precision: 
  
    ![equation](https://latex.codecogs.com/gif.latex?%5Cfrac%7BVP%7D%7BVP&plus;FP%7D)
    
  - Recall (o sensibilidad): 
  
    ![equation](https://latex.codecogs.com/gif.latex?%5Cfrac%7BVP%7D%7BVP&plus;FN%7D)
    
  - F1-score: 
  
    ![equation](https://latex.codecogs.com/gif.latex?%5Cfrac%7B%202%20%5Ccdot%20%28Recall%20%5Ccdot%20Precision%29%7D%7B%28Recall%20&plus;%20Precision%29%7D)

  Las m√©tricas de desempe√±o en las clasificaciones se escogen dependiendo del contexto del problema a resolver.

### üîª Curvas de aprendizaje

A partir de las m√©tricas descritas anteriormente, se construyen las curvas de aprendizaje. En ellas podemos evaluar si el algoritmo logr√≥ generalizar los datos, o por el contrario, necesita a√±adirle complejidad o reducirla. Estas caracter√≠sticas son apreciadas al graficar la m√©trica escogida en funci√≥n del n√∫mero de iteraciones (o √©pocas), donde podemos encontrarnos con los siguientes escenarios: 

- Underfitting (o sub ajuste): cuando el modelo es incapaz de obtener resultados correctos por falta de entrenamiento o de m√°s muestras. Se reconoce visualmente cuando existe altos valores de p√©rdida tanto en el set de entrenamiento como en el de validaci√≥n. 

- Overfitting (o sobre ajuste): cuando el modelo se ajusta solo a los datos de entrenamiento y se vuelve incapaz de reconocer nuevos datos. Es visualmente reconocido cuando existe una separaci√≥n importante entre las p√©rdidas del set de entrenamiento y el set de validaci√≥n. 

- Optimal fit: cuando el modelo logra captar el comportamiento general de los datos. Se puede identificar al obtener valores de p√©rdidas en los datos de validaci√≥n que no difieren demasiado de las p√©rdidas de los datos de entrenamiento. 

<p align="center">
  <img width="700" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff80030c0-6072-4d13-a6d7-e148f6c5c39a%2FUntitled.png?table=block&id=4c389bbb-7692-43aa-b4e8-016b1a14d03b&width=2050&userId=5d54f20c-b387-4e7e-b2a5-d88e235ada88&cache=v2">
</p>

La forma general de resolver el problema de underfitting es a√±adiendo complejidad a la red. Esto puede llevarse a cabo al incluir m√°s datos o al aumentar el entrenamiento de la red. Para el problema del overfitting es recomendable reducir la cantidad de variables o a√±adir [t√©cnicas de regularizaci√≥n](https://www.notion.so/mariajosemv/Redes-neuronales-en-Keras-y-ScikitLearn-b8fcf479b0464021bb85d1b2a8863404#150430fc557048d7820017da9ca66ea5), las cuales consisten en en disminuir la complejidad del modelo por medio de una penalizaci√≥n aplicada a sus variables m√°s irrelevantes. 

----

## üìå Notas 

Los proyectos fueron construidos utilizando las librer√≠as [Scikit-Learn](https://scikit-learn.org/) y [Keras](https://keras.io/), ambos considerados frameworks de alto nivel, orientados a la experiencia de usuario y caracterizados por permitir realizar experimentaciones r√°pidas. 

Cada proyecto fue realizado como parte del [Curso de Redes Neuronales en Keras y Scikit-Learn](https://platzi.com/cursos/keras-neural-networks/) de Platzi. Para m√°s informaci√≥n y detalles, te invito a leer mis notas del curso en [Notion](https://www.notion.so/mariajosemv/Redes-neuronales-en-Keras-y-ScikitLearn-b8fcf479b0464021bb85d1b2a8863404). ‚ú®
